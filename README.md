# ğŸ¤–ğŸ§ ğŸŒ How Do Large Language Models Like GPT Work? â€“ Explained for Everyone!

Welcome to this beginner-friendly repository that **demystifies how Large Language Models (LLMs)** like **GPT**, **Gemini**, and others work â€“ step by step! âœ¨

If youâ€™ve ever wondered *"How can a machine generate text so well?"*, you're in the right place. This repo breaks down the key concepts behind LLMs in simple code and explanations. No AI degree needed! ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ«

---

## ğŸ“š Repository Structure

Each folder in this repo is a **building block** that takes you closer to understanding how LLMs operate:

### ğŸ§© `01-byte-pair-encoding/`
**Concept:** Tokenization with **Byte Pair Encoding (BPE)**  
ğŸ“– [Read the README](./01-byte-pair-encoding/README.md)  
ğŸ› ï¸ Code: `bpe.py`  
Learn how words are split into subword units to make models efficient and flexible with language.

---

### ğŸ” `02-bi-gram-models/`
**Concept:** Basic **Bi-gram Language Models**  
ğŸ“– [Read the README](./02-bi-gram-models/README.md)  
ğŸ› ï¸ Code: `bi_gram.py`  
ğŸ“ Sample Data: `data/sample_data.txt`  
Understand how simple statistical models predict the next word using word pairs.

---

### ğŸ”¥ `03-transformers/`
**Concept:** The magic of **Transformer models** (like GPT!)  
ğŸ“– [Read the README](./03-transformers/README.md)  
ğŸ› ï¸ Code: `gpt_demo.py`  
See how the Transformer architecture powers state-of-the-art language models.

---

## ğŸš€ Who Is This For?

This project is perfect for:
- ğŸ§‘â€ğŸ“ Students curious about AI
- ğŸ› ï¸ Developers wanting to peek under the hood
- ğŸ§  Enthusiasts who enjoy learning how things work

No advanced math or machine learning knowledge required. Just basic Python and curiosity! ğŸğŸ’¡

---

## ğŸ’¬ Why This Matters

Understanding LLMs helps us:
- Make informed decisions about AI usage
- Appreciate the complexity of tools like ChatGPT
- Build our own simple models from scratch!

---

## ğŸ“œ License

This project is licensed under the MIT License.